import config
import DreamingNets
import numpy as np
import hockey.hockey_env as h_env
import gymnasium as gym
import torch
import os
from torch import nn
from PIL import Image
from torch.nn import functional as F
from ModifyHockeyEnv import ModifiedHockeyEnv

'''Cuda stuff'''
torch.set_default_dtype(torch.float32)
torch.autograd.set_detect_anomaly(True)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
torch.set_default_device(device)
print("Device: " + str(device))

class WorldModel:
    def __init__(self):
        self.autoencoder = DreamingNets.BinaryAutoencoder()
        self.sequence_predictor = DreamingNets.SequencePredictor()
        self.reward_net = DreamingNets.reward_net()
        self.continuation_net = DreamingNets.continuation_net()
        self.dynamics_predictor = DreamingNets.DynamicsPredictor()

        self.sequence_predictor.apply(self.initialize_weights)
        self.reward_net.apply(self.initialize_weights)
        self.continuation_net.apply(self.initialize_weights)
        self.dynamics_predictor.apply(self.initialize_weights)
        self.autoencoder.apply(self.initialize_weights)

        self.parameters = list(self.autoencoder.parameters()) + list(self.sequence_predictor.parameters()) + list(self.reward_net.parameters()) + list(self.continuation_net.parameters()) + list(self.dynamics_predictor.parameters())
        self.optimizer = config.OPTIMIZER(self.parameters, lr=config.LR)
        self.scheduler = config.SCHEDULER(self.optimizer, step_size=10, gamma=0.9)

        self.h = torch.zeros(config.SNET_NUM_LAYERS, config.H_SIZE)
        self.next_h = torch.zeros(config.SNET_NUM_LAYERS, config.H_SIZE)
        self.z = torch.zeros(config.Z_SIZE, requires_grad=True)
        self.c_next = torch.zeros(1)
        self.dream_space = []

        self.steps = 0

        # DreamSpace:
        # A list of lists. Each list is a dream sequence.
        # Each dream sequence is a list of tuples (h, z, r).
        # The first sequence is the oldest dream.

        self.actor = h_env.BasicOpponent(weak=False)

        print('World model initialized')

    # helps initializing the weights of the networks
    def initialize_weights(self, layer):
        if isinstance(layer, nn.Linear):
            nn.init.xavier_uniform_(layer.weight)
            nn.init.zeros_(layer.bias)


    def dream(self, c):
        # Dreaming
        dreamed_sequence = []
        h_prev = self.h
        z_prev = self.z
        c = torch.tensor([c], dtype=torch.float32)

        for i in range(config.DREAM_STEPS):
            obs_dec = self.autoencoder.decode(z_prev)
            a = torch.tensor(self.actor.act(obs_dec.cpu().detach().numpy()), dtype=torch.float32)

            _, h_next = self.sequence_predictor.forward(z_prev, a, c, h_prev)
            z = self.dynamics_predictor.forward(h_next[-1])
            r = self.reward_net.forward(z.unsqueeze(0))
            c = self.continuation_net.forward(z.unsqueeze(0))

            dreamed_sequence.append((h_next.clone(), z.clone(), r.clone(), c.clone()))

            h_prev = h_next
            z_prev = z
            if i == 0:
                self.h_next = h_next
                self.c_next = c
        self.dream_space.append(dreamed_sequence)


    
    def computeDreamLosses(self, z_dream, z_enc, r_pred, r, c_pred, c):
        loss = 0

        # Normalize rewards
        #r = r / 10

        r = torch.tensor([r], dtype=torch.float32)
        c = torch.tensor([c], dtype=torch.float32)
        z_enc_sg = z_enc.clone().detach()
        z_dream_sg = z_dream.clone().detach()

        # Reward loss
        loss += self.reward_net.computeLoss(r_pred, r) * config.REWARD_SCALE
        # Continuation loss
        loss += self.continuation_net.computeLoss(c_pred, c) * config.CONTINUATION_SCALE * 0.1
        # Dynamic and Representation loss
        loss_rep = config.dLoss(z_enc, z_dream_sg)    # Adapts encoder to Worldmodel
        loss_dyn = config.dLoss(z_enc_sg, z_dream)    # Adapts Worldmodel to encoder
        loss += loss_rep * config.REP_SCALE + loss_dyn * config.DYNAMICS_SCALE
        
        return loss


    def computeStepLoss(self, obs, c):
        loss = 0
        # Reconstruction loss of the Autoencoder
        loss += self.autoencoder.computeLoss(obs) * config.AUTOENCODER_SCALE

        # Contiunation loss
        z_enc, _ = self.autoencoder.forward(obs)
        c_pred = self.continuation_net.forward(z_enc).unsqueeze(0)
        c = torch.tensor([c], dtype=torch.float32)
        closs = self.continuation_net.computeLoss(c_pred, c) * config.CONTINUATION_SCALE * 2
        if c_pred > 0.5:
            print(f'goal predicted with {c_pred} at step {self.steps}')
        loss += closs
        return loss
    

    def trainingStep(self, obs, r, c, print_loss=False):
        # Training step
        z_enc, _ = self.autoencoder.forward(obs)
        total_loss = torch.tensor(0.0)
        new_dream_space = []

        step_loss = self.computeStepLoss(obs, c)

        for seq in self.dream_space:
                if not seq:
                    continue
                
                h_pred, z_pred, r_pred, c_pred = seq[0]
                seq = seq[1:]
                
                dream_loss = self.computeDreamLosses(z_pred, z_enc, r_pred, r, c_pred, c) * np.power(config.DISCOUNT, 20 - len(seq))
                total_loss = total_loss + dream_loss
                
                if seq:
                    new_dream_space.append(seq)

        if print_loss:
            print("Step loss: " + str(step_loss) + " Dream loss: " + str(total_loss))

        total_loss = total_loss + step_loss

        self.dream_space = new_dream_space
        self.dream(c)
        self.steps += 1
        return total_loss
    

    def trainingRound(self, env, player2):
        total_loss = torch.tensor(0.0)
        self.h = torch.zeros(config.SNET_NUM_LAYERS, config.H_SIZE)

        #Setup environment
        obs, info = env.reset()
        obs_agent2 = env.obs_agent_two()
        obs_t = torch.tensor(obs, dtype=torch.float32, requires_grad=True)
        obs_t = self.normalize(obs_t)
        self.z, _ = self.autoencoder.forward(obs_t)
        c = torch.tensor([0], dtype=torch.float32)

        #First initializations
        self.dream(c)

        #Train
        for i in range(config.TRAIN_STEPS):
            #get next state
            if c: 
                print(f'Goal achived, resetting || Prediction: {self.c_next} at step {i}')
                # --------- Reset h aswell????? ---------
                env.reset()
            a1 = self.actor.act(obs)
            a2 = player2.act(obs_agent2)
            obs, r, c, t, info = env.step(np.hstack([a1,a2]))
            obs_agent2 = env.obs_agent_two()
            if t: break

            obs_t = torch.tensor(obs, dtype=torch.float32)
            obs_t = self.normalize(obs_t)
            self.z, _ = self.autoencoder.forward(obs_t)     # needed?
            self.h = self.h_next                            # next latent state as current latent state
            
            total_loss = total_loss + self.trainingStep(obs_t, r, c, print_loss=(i % config.UPDATE_INTERVAL == 0 and False))

            if i % config.UPDATE_INTERVAL == 0 and i != 0:
                with open(config.LOG_FILE_NAME, "a") as log_file:
                    log_line = f"{self.steps}, {total_loss.item():.6f}\n"
                    log_file.write(log_line)
                    log_file.flush()
                print('Step: ' + str(i) + ' Loss: ' + str(total_loss))
                self.optimizer.zero_grad()
                total_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.parameters, max_norm=0.9)
                self.optimizer.step()
                self.scheduler.step()

                total_loss = torch.tensor(0.0, requires_grad=True)
                self.full_reset()

        # Use remaining data
        print('Step: ' + str(i) + ' Loss: ' + str(total_loss))
        self.optimizer.zero_grad()
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.parameters, max_norm=0.9)
        self.optimizer.step()

        # Reset everything (I dont really need to do that since this is the end of the training round anyway but just in case ._.)
        total_loss = torch.tensor(0.0, requires_grad=True)
        self.full_reset()
        env.close()
        

    def full_reset(self):
        # Reset everything
        self.dream_space = []
        self.h = self.h.detach()
        self.z = self.z.detach()            #not really needed but just to be sure
        self.h_next = self.h_next.detach()  #not needed either, since it gets overwritten before its used anyway
    

    '''Not sure why i even have this function right now but im sure it will be useful later on'''
    def trainIteration(self):
        env = h_env.HockeyEnv()
        player2 = h_env.BasicOpponent(weak=True)
        self.steps = 0

        for i in range(config.TRAIN_SEQUENCES):
            self.trainingRound(env, player2)
            print(f'Sequence {i} done')
        print('Training done')
        self.saveModel()


    def normalize(self, x):
        # x, mean, and std should be torch tensors
        mean = torch.tensor(config.MEAN, dtype=torch.float32, device=device)
        std = torch.tensor(config.STD, dtype=torch.float32, device=device)
        return (x - mean) / std

    def denormalize(self, x_norm):
        mean = torch.tensor(config.MEAN, dtype=torch.float32, device=device)
        std = torch.tensor(config.STD, dtype=torch.float32, device=device)
        return x_norm * std + mean

    
    def saveModel(self):
        whole_model = {'sequence_predictor': self.sequence_predictor.state_dict(),
                       'reward_net': self.reward_net.state_dict(),
                       'dynamics_predictor': self.dynamics_predictor.state_dict(),
                       'continuation_net': self.continuation_net.state_dict(),
                       'autoencoder': self.autoencoder.state_dict()}
        torch.save(whole_model, config.FILE_NAME)
        print('Models saved')


    def loadModel(self):
        #check for file 
        if not os.path.isfile(config.FILE_NAME):
            print('No model found')
            return
        print(f"Loading model {config.FILE_NAME}")
        whole_model = torch.load(config.FILE_NAME)
        self.sequence_predictor.load_state_dict(whole_model['sequence_predictor'])
        self.reward_net.load_state_dict(whole_model['reward_net'])
        self.dynamics_predictor.load_state_dict(whole_model['dynamics_predictor'])
        self.continuation_net.load_state_dict(whole_model['continuation_net'])
        self.autoencoder.load_state_dict(whole_model['autoencoder'])
        print('Models loaded')

    
    def loadAutoencoder(self):
        #check for file 
        if not os.path.isfile(config.AE_FILE_NAME):
            print('No model found')
            return
        print(f"Loading model {config.AE_FILE_NAME}")
        model = torch.load(config.AE_FILE_NAME)
        self.autoencoder.load_state_dict(model)
        print('Autoencoder loaded')
        '''for param in self.autoencoder.parameters():
            param.requires_grad = False
        print('Autoencoder frozen')'''


    # ----------------- Test -----------------
    def test_run(self):
            # Create directory for saved images
        saved_images_dir = "./test_images/"
        os.makedirs(saved_images_dir, exist_ok=True)

        # Initialize environment and opponent
        env = ModifiedHockeyEnv()
        obs, info = env.reset()
        obs_agent2 = env.obs_agent_two()
        player2 = h_env.BasicOpponent(weak=True)

        # Initialize latent state and context (c)
        obs_tensor = torch.tensor(obs, dtype=torch.float32)
        obs_tensor = self.normalize(obs_tensor)
        self.z, _ = self.autoencoder.forward(obs_tensor)
        self.h = torch.zeros(config.SNET_NUM_LAYERS, config.H_SIZE)
        c = torch.tensor([0], dtype=torch.float32)

        # Perform an initial dream to populate the dreamspace
        self.dream(c)

        # Choose 5 random steps (out of 100) to save renders
        random_steps = sorted(np.random.choice(range(100), size=5, replace=False))
        print("Saving images for steps:", random_steps)

        # Run simulation for 100 steps
        for step in range(100):

            # Save images for selected steps
            if step in random_steps:
                # Save the "normal" render of the current environment state
                normal_img = env.render(mode='rgb_array')

                # Autoencoded render: Encode and decode the current observation
                obs_tensor = torch.tensor(obs, dtype=torch.float32)
                normalized_obs = self.normalize(obs_tensor)
                latent, _ = self.autoencoder.forward(normalized_obs)
                reconstructed_obs = self.autoencoder.decode(latent)
                print("Reconstruction difference:", reconstructed_obs - normalized_obs)
                env.set_state(reconstructed_obs.cpu().detach().numpy().astype(np.float64))
                autoencoded_img = env.render(mode='rgb_array')

                # Dreamed renders: Decode all 15 imagined latents for this step
                dreamed_imgs = []
                for seq in self.dream_space:
                    dreamed_latent, _, _, _ = seq[0]  # Extract latent from dreamspace
                    dreamed_z = self.dynamics_predictor.forward(dreamed_latent[-1])
                    dreamed_decoded = self.autoencoder.decode(dreamed_z)
                    env.set_state(dreamed_decoded.cpu().detach().numpy().astype(np.float64))
                    dreamed_imgs.append(env.render(mode='rgb_array'))

                # Save images with filenames starting with the step number
                step_str = f"{step:03d}"
                normal_filename = os.path.join(saved_images_dir, f"{step_str}_anormal.png")
                autoenc_filename = os.path.join(saved_images_dir, f"{step_str}_autoencoded.png")
                Image.fromarray(normal_img).save(normal_filename)
                Image.fromarray(autoencoded_img).save(autoenc_filename)

                for i, dreamed_img in enumerate(dreamed_imgs):
                    dreamed_filename = os.path.join(saved_images_dir, f"{step_str}_dreamed_{i:02d}.png")
                    Image.fromarray(dreamed_img).save(dreamed_filename)

                print(f"Step {step}: Saved normal, autoencoded, and dreamed renders.")

            # Get actions for both agents
            action_agent1 = self.actor.act(obs)
            action_agent2 = player2.act(obs_agent2)

            # Step the environment with combined actions
            obs, reward, continuation, terminated, info = env.step(np.hstack([action_agent1, action_agent2]))
            obs_agent2 = env.obs_agent_two()

            if terminated:
                print(f"Environment terminated at step {step}.")
                break

            # Update latent state with new observation
            obs_tensor = torch.tensor(obs, dtype=torch.float32)
            obs_tensor = self.normalize(obs_tensor)
            self.z, _ = self.autoencoder.forward(obs_tensor)

            # Optionally perform a training step (if needed during testing)
            self.trainingStep(obs_tensor, reward, continuation)


            # Debugging logs for latent differences and losses (optional)
            if step % config.UPDATE_INTERVAL == 0:
                for seq in self.dream_space:
                    if not seq:
                        continue
                    h_pred, z_pred, r_pred, c_pred = seq[0]
                    diff_z = z_pred - self.z
                    #print(f"Step {step} - Latent Differences: {diff_z}")
                    loss_dream = self.computeDreamLosses(z_pred, self.z, r_pred, reward, c_pred, continuation)
                    print(f"Step {step} - Dream Loss: {loss_dream}")

                decoded_obs = self.autoencoder.decode(self.z)
                reconstruction_diff = decoded_obs - obs_tensor
                print(f"Step {step} - Reconstruction Differences: {reconstruction_diff}")
                print(f"Step {step} - Autoencoder Loss: {self.autoencoder.computeLoss(obs_tensor)}")

        print("Test run completed.")

if __name__ == '__main__':
    world = WorldModel()
    world.loadModel()
    #world.loadAutoencoder()
    #world.trainIteration()
    world.test_run()
